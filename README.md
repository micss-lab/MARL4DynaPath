# Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments
This project contains the implementation of a multi-agent reinforcement learning (MARL) system for path planning in dynamic environments. The implementation is built using C++ and uses Python for visualizing the results. The implementation is mostly divided into classes that represent different components of the system, such as the A* algorithm, the environment, the learning algorithms, and the visualization tool. Our implementation supports parallel processing of different sib-environments, as well as the parallel processing of agents in a single sub-environment, used by the federated Q-learning algorithm. 

## Installing the project
1. To install the project, either fork it directly from GitHub or download it as a zip file via the green "Code" button on the top right of the repository page. If you choose to download it as a zip file, extract the contents to your desired location.
2. Navigate to the project root directory in your terminal.

## Project Structure

```
project-root/
|
|-- plots/                          # results.csv + results_detailed.csv + plots generated by the regular experiments.
|-- plots-edge-case/                # results.csv + results_detailed.csv + plots generated by the edge case experiment.
|-- src/                            # Source code of this project.
|   |-- astar.(h|cpp)               # A* algorithm for pathfinding in the complete environment.
|   |-- constants.h                 # Constant values used throughout the implementation.
|   |-- experiments.(h|cpp)         # Simulation of environment changes and the experiment setup.
|   |-- hashpair.(h|cpp)            # HashPair class, used in the A* algorithm to efficiently store and retrieve found paths.
|   |-- main.cpp                    # Calls the function to run the experiments.
|   |-- maze.(h|cpp)                # MDP (Markov Decision Process) implementation of the maze environment.
|   |-- multiagent.(h|cpp)          # Federated Q-learning implementation (fedAsynQ_EqAvg and fedAsynQ_ImAvg).
|   |-- pathstate.h                 # PathState class, used when constructing paths to a charging station.
|   |-- policyvisualizer.(h|cpp)    # PolicyVisualizer class, used to visualize the policies of the agents in the environment.
|   |-- singleagent.(h|cpp)         # Single agent Q-learning implementation.
|   |-- startstats.(h|cpp)          # StartStats class, used when selecting the starting positions of the agents (prioritized replay).
|   |-- table.(h|cpp)               # Table class, used as the Q-table for the agents (three dimensional vector).
|   |-- testpolicy.(h|cpp)          # Test the learned policy of the agents in the environment.
|   |-- threadresult.h              # ThreadResult class, used to store the results of threads created for parallel learning of agents.
|   |-- treenode.(h|cpp)            # TreeNode class, representing a node in the hierarchical tree.
|   |-- treestrategy.(h|cpp)        # TreeStrategy class, implementing the hierarchical tree strategy and the parallel processing of tree nodes.
|   |-- visualizations.py           # Python script to visualize the results of the experiments.
|-- CMakeLists.txt                  # CMake build configuration file.
|-- README.md                       # Project overview and instructions to run experiments.
|-- arial.ttf                       # Font file used in the PolicyVisualizer class to visualize policies.
```

## Prerequisites
1. Ensure you have a C++ compiler installed on your system. This project has been tested with the g++ compiler, which is part of the GNU Compiler Collection (GCC). If you don't have it installed, you can install it via your package manager. On Ubuntu, you can install it with the following command:
   ```shell
   sudo apt-get install g++
   ```
2. Ensure you have CMake installed on your system. If you are using the CLion IDE, it comes with CMake pre-installed. If you are using a different IDE or the command line, you can install CMake via your package manager. On Ubuntu, you can install it with the following command:
    ```shell
    sudo apt-get install cmake
    ```

## Installing dependencies
1. Install the SFML library, which is used for visualizing the learned policies of the agents in the environment. The SFML library is a cross-platform multimedia library that provides a simple interface to various components such as graphics, audio, and network. You can install it via your package manager or download it from the [SFML website](https://www.sfml-dev.org/download.php). On Ubuntu, you can install it with the following command:
   ```shell
    sudo apt-get install libsfml-dev
   ```
   Ensure that version 2.6 or higher is installed, as the project requires features from this version.

2. Install the Python dependencies for visualizing the results. The visualization script requires the following Python packages:
   - `matplotlib`
   - `pandas`
    
   You can install these packages using pip:
   ```shell
   pip install matplotlib pandas
   ```
### Demo video
...


## Running the experiments
---
...

### Instructions
...

### Demo video
...

### Visualizing the results
...

#### Instructions
...


## Running the edge case experiment
---
...

### Instructions
...

### Demo video
...

### Visualizing the results
...

#### Instructions
...


## Enabling the policy tracker
---
...

### Instructions
...

### Demo video
...
